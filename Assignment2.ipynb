{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate Change Misinformation Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Reading the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External data from Buzzfeed has been added as negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_data_from_file(filepath,ty):\n",
    "    with open(filepath) as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    size = len(data)\n",
    "    for i in range(size):\n",
    "        sentences.append(data[ty+'-'+str(i)]['text'])\n",
    "        if'label' in data[ty+'-'+str(i)].keys():\n",
    "            labels.append(data[ty+'-'+str(i)]['label'])\n",
    "    return sentences,labels \n",
    "\n",
    "train_sents_posi,y_train_posi = get_data_from_file(\"train.json\",\"train\")\n",
    "train_added,y_train_added = get_data_from_file(\"buzzfeed.json\",\"train\")\n",
    "train_sents = train_sents_posi + train_added       \n",
    "y_train = y_train_posi + y_train_added\n",
    "\n",
    "dev_sents,y_dev = get_data_from_file(\"dev.json\",\"dev\")\n",
    "test_sents,y_test = get_data_from_file(\"test-unlabelled.json\",\"test\")\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "print(len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from spacy.lang.en import English \n",
    "# Preprocess the documents\n",
    "# Tokenize the documents and remove the stopwords and find lemma\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Preprocess the data\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "  \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.tag.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = lemmatizer.lemmatize(word.lower(),get_wordnet_pos(word))\n",
    "    return lemma\n",
    "      \n",
    "def get_token(text):\n",
    "    processed_tokens = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token.lower() in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            processed_tokens.append(token)\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def preprocess_data(text):\n",
    "    tokens = get_token(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Topic Modelling on the training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "clean_doc = [preprocess_data(doc) for doc in train_sents]\n",
    "dictionary = corpora.Dictionary(clean_doc)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time --- 459.09 seconds \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "\n",
    "start_time = time.time()\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)\n",
    "train_time = time.time() - start_time\n",
    "print(\"Training Time --- %s seconds \" % (round(train_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.25125414101154\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', ldamodel.log_perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.025*\"climate\" + 0.013*\"change\" + 0.007*\"global\" + 0.007*\"warm\" + 0.007*\"energy\" + 0.006*\"temperature\" + 0.005*\"would\" + 0.005*\"carbon\" + 0.005*\"emission\" + 0.005*\"science\" \n",
      "\n",
      "1   0.034*\"clinton\" + 0.034*\"trump\" + 0.015*\"debate\" + 0.012*\"hillary\" + 0.011*\"campaign\" + 0.008*\"donald\" + 0.008*\"state\" + 0.007*\"presidential\" + 0.006*\"voter\" + 0.006*\"republican\" \n",
      "\n",
      "2   0.008*\"obama\" + 0.008*\"state\" + 0.007*\"president\" + 0.007*\"would\" + 0.005*\"country\" + 0.005*\"security\" + 0.005*\"court\" + 0.004*\"people\" + 0.004*\"government\" + 0.004*\"syria\" \n",
      "\n",
      "3   0.017*\"police\" + 0.008*\"people\" + 0.007*\"officer\" + 0.007*\"adult\" + 0.007*\"black\" + 0.006*\"hawkins\" + 0.006*\"thing\" + 0.005*\"charlotte\" + 0.005*\"young\" + 0.004*\"video\" \n",
      "\n",
      "4   0.028*\"trump\" + 0.010*\"republican\" + 0.008*\"would\" + 0.007*\"donald\" + 0.007*\"president\" + 0.007*\"photo\" + 0.006*\"people\" + 0.005*\"white\" + 0.005*\"caption\" + 0.005*\"national\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print all the topics\n",
    "for topic in ldamodel.print_topics(num_topics=5, num_words=10):\n",
    "    print(topic[0],\" \",topic[1],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6309511), (2, 0.1567901), (3, 0.08145625), (4, 0.12921995)]\n"
     ]
    }
   ],
   "source": [
    "# Get the topic of the document by the trained model\n",
    "def categorize_topic(doc):\n",
    "    clean_for_lda = preprocess_data(doc)\n",
    "    doc_bow = dictionary.doc2bow(clean_for_lda)\n",
    "    topic = ldamodel.get_document_topics(doc_bow)\n",
    "    return topic\n",
    "\n",
    "def classify_topic(docs):\n",
    "    labels = {}\n",
    "    for i in range(len(docs)):\n",
    "        topic = categorize_topic(docs[i])\n",
    "        if len(topic) < 2:\n",
    "            if topic[0][0] != 0:\n",
    "                labels[i] = 0\n",
    "        else:\n",
    "            max = 0\n",
    "            for j in range(1,len(topic)):\n",
    "                if topic[j][1] > topic[max][1]:\n",
    "                    max = j\n",
    "            if max != 0:\n",
    "                labels[i] = 0\n",
    "                \n",
    "    return labels\n",
    "\n",
    "\n",
    "dev_topic_label = classify_topic(dev_sents)\n",
    "test_topic_label = classify_topic(test_sents)\n",
    "#print(test_topic_label)\n",
    "\n",
    "\n",
    "print(categorize_topic(test_sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the remaining unclassified documents\n",
    "def get_the_rest_docs(original_docs,assigned_topics):\n",
    "    docs_idx = []\n",
    "    docs = []\n",
    "    for i in range(len(original_docs)):\n",
    "        if i not in assigned_topics.keys():\n",
    "            docs_idx.append(i)\n",
    "            docs.append(original_docs[i])\n",
    "    return docs_idx, docs\n",
    "        \n",
    "remaining_dev_idx, remaining_dev = get_the_rest_docs(dev_sents,dev_topic_label)\n",
    "remaining_test_idx, remaining_test = get_the_rest_docs(test_sents,test_topic_label)\n",
    "#print(len(remaining_dev_idx))\n",
    "#print(remaining_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. 10. ...  0.  0.  0.]\n",
      " [ 0.  0. 60. ...  0.  0.  0.]\n",
      " [ 0.  0. 32. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0. 83. ...  0.  0.  0.]\n",
      " [ 0.  0. 11. ...  0.  0.  0.]\n",
      " [ 0.  0. 32. ...  1.  1.  1.]]\n",
      "Vocab size = 48567\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "test1 = train_sents\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(test1)\n",
    "x_train = tokenizer.texts_to_matrix(test1, mode=\"count\") \n",
    "x_dev = tokenizer.texts_to_matrix(remaining_dev, mode=\"count\")\n",
    "x_test = tokenizer.texts_to_matrix(remaining_test, mode=\"count\")\n",
    "\n",
    "print(x_train)\n",
    "vocab_size = x_train.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_dev_remaining = classifier.predict(x_dev)\n",
    "#score = classifier.score(x_dev, y_dev)\n",
    "\n",
    "y_test_remaining = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"feedforward-bow-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                485680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 485,691\n",
      "Trainable params: 485,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fit a feed forward neural network model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "vocab_size = xx_test.shape[1]\n",
    "model = Sequential(name=\"feedforward-bow-input\")\n",
    "model.add(layers.Dense(10, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2548 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "2548/2548 [==============================] - 4s 1ms/step - loss: 0.1142 - accuracy: 0.9776 - val_loss: 2.8540 - val_accuracy: 0.7200\n",
      "Epoch 2/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0165 - accuracy: 0.9992 - val_loss: 3.3358 - val_accuracy: 0.7100\n",
      "Epoch 3/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.8636 - val_accuracy: 0.6900\n",
      "Epoch 4/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.9614 - val_accuracy: 0.6900\n",
      "Epoch 5/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.0277 - val_accuracy: 0.6900\n",
      "Epoch 6/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0915 - val_accuracy: 0.6800\n",
      "Epoch 7/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1278 - val_accuracy: 0.6800\n",
      "Epoch 8/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 8.6184e-04 - accuracy: 1.0000 - val_loss: 4.1599 - val_accuracy: 0.6800\n",
      "Epoch 9/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 6.6983e-04 - accuracy: 1.0000 - val_loss: 4.1914 - val_accuracy: 0.6700\n",
      "Epoch 10/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 5.3009e-04 - accuracy: 1.0000 - val_loss: 4.2088 - val_accuracy: 0.6700\n",
      "Epoch 11/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 4.2582e-04 - accuracy: 1.0000 - val_loss: 4.2379 - val_accuracy: 0.6700\n",
      "Epoch 12/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 3.4530e-04 - accuracy: 1.0000 - val_loss: 4.2431 - val_accuracy: 0.6700\n",
      "Epoch 13/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 2.8240e-04 - accuracy: 1.0000 - val_loss: 4.2583 - val_accuracy: 0.6700\n",
      "Epoch 14/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 2.3312e-04 - accuracy: 1.0000 - val_loss: 4.2761 - val_accuracy: 0.6700\n",
      "Epoch 15/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 1.9354e-04 - accuracy: 1.0000 - val_loss: 4.2897 - val_accuracy: 0.6700\n",
      "Epoch 16/20\n",
      "2548/2548 [==============================] - 4s 1ms/step - loss: 1.6174e-04 - accuracy: 1.0000 - val_loss: 4.3012 - val_accuracy: 0.6700\n",
      "Epoch 17/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 1.3538e-04 - accuracy: 1.0000 - val_loss: 4.3179 - val_accuracy: 0.6700\n",
      "Epoch 18/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 1.1423e-04 - accuracy: 1.0000 - val_loss: 4.3345 - val_accuracy: 0.6700\n",
      "Epoch 19/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 9.6581e-05 - accuracy: 1.0000 - val_loss: 4.3421 - val_accuracy: 0.6700\n",
      "Epoch 20/20\n",
      "2548/2548 [==============================] - 3s 1ms/step - loss: 8.1601e-05 - accuracy: 1.0000 - val_loss: 4.3468 - val_accuracy: 0.6700\n",
      "\n",
      "Testing Accuracy:  0.6700\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model.fit(xx_test, y_train, epochs=20, verbose=True, validation_data=(xx_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(xx_dev, y_dev, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = model.predict(x_test)\n",
    "y_result = y_result.tolist()\n",
    "y_test = []\n",
    "for it in y_result:\n",
    "    for i in it:\n",
    "        y_test.append(round(i))\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the result labels\n",
    "def collect_labels(docs_length,topic_label,classified_idx,classified_label):\n",
    "    y_result = [1]*docs_length\n",
    "    for i in range(len(y_result)):\n",
    "        if i in topic_label.keys():\n",
    "            y_result[i] = topic_label[i]\n",
    "        if i in classified_idx:\n",
    "            y_result[i] = classified_label[classified_idx.index(i)]\n",
    "    return y_result\n",
    "\n",
    "y_dev_result = collect_labels(len(dev_sents),dev_topic_label,remaining_dev_idx, y_dev_remaining)\n",
    "#y_test = collect_labels(len(test_sents),test_topic_label,remaining_test_idx,y_test_remaining)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "        \n",
    "def output_file(cat,labels):\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    for i in range(len(labels)):\n",
    "        dict2[\"label\"] = labels[i]\n",
    "        emu = cat + \"-\" + str(i)\n",
    "        dict1[emu] = dict(dict2)\n",
    "    out_file = open(cat + \"-output.json\",\"w\")\n",
    "    json.dump(dict1,out_file,cls=NpEncoder)\n",
    "    out_file.close()\n",
    "    \n",
    "    \n",
    "#output_file(\"test\",y_test)\n",
    "output_file(\"dev\",y_dev_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
